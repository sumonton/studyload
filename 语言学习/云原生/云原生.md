# 一、云平台

## 1、为什么用云平台

* 环境统一
* 按需付费
* 即开即用
* 稳定性强
* 国内常见云平台
  * 阿里云、百度云、腾讯云、华为云、青云
* 国外常见云平台
  * 亚马逊AWS、微软Azure

### 1.1 公有云

* 成本更低：无需购买硬件或软件，仅对使用的服务付费
* 无需维护：维护有服务提供商提供
* 近乎无限制的缩放性：提供按需资源，可满足业务需求
* 高可靠性：具备众多服务器，确保免受故障影响
  * 可用性：N个9 全年故障时间：365\*34\*36000\*(1-99.9999%)

### 1.2 私有云

* 灵活性更强：组织可自定义云环境以满足特定业务需求
* 控制力更强：资源不与其他组织共享，因此能获得更高的控制力以及更高的隐私级别
* 可伸缩性更强：与本地基础结构相比，私有云通产更具有更强的可伸缩性

## 2、云平台操作

* 云平台安全组：即防火墙

* VPC

  ![image-20240108000705428](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/202401080007524.png)

# 二、Docker基础

## 1、Docker基本概念

### 1.1 解决的问题

#### 1.1.1 统一标准

* 应用构建
  * Java、C++、JavaScript
  * 打成软件包
  * .exe
  * Docker build ... 镜像
* 应用分享
  * 所有软件的镜像放到一个指定地方 docker hub
  * 安卓，应用市场
* 应用运行
  * 统一标准的镜像
  * docker run

#### 1.1.2 资源隔离

* CPU、memory资源隔离与限制
* 访问设备隔离与限制
* 网络隔离与限制
* 用户、用户组隔离限制

### 1.2 架构

![image-20240108001824716](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/202401080018776.png)

* Docker_Host
  * 安装Docker的主机
* Docker Daemon
  * 运行在Docker主机上的Docker后台进程
* Client
  * 操作Docker主机的客户端（命令行、UI等）
* Registry
  * 镜像仓库
  * Docker Hub
* Images
  * 镜像，带黄精打包好的程序，可以直接启动运行
* Containers
  * 容器，由镜像启动起来正在运行中的程序

### 1.3 安装

#### 1.3.1 配置加速

* 配置国内镜像和生产环境核心配置cgroup

  ```json
  sudo mkdir -p /etc/docker
  sudo tee /etc/docker/daemon.json <<-'EOF'
  {
    "registry-mirrors": ["https://oluas55l.mirror.aliyuncs.com"]
  }
  EOF
  sudo systemctl daemon-reload
  sudo systemctl restart docker
  ```

## 2、docker命令实战

## 2.1 找镜像

* 去docker市场[docker hub](https://hub.docker.com/)下载镜像

```bash
docker pull nginx

docker pull redis:7.4.2

#删除镜像
docker rmi 
```

## 2.2 启动容器

```bash
docker run 设置项 镜像名

# -d 后台运行
# --restart=always 开机自启
# -p 88:80 端口映射

# 查看正在运行的容器
docker ps
#查看所有
docker ps -a
#删除停止的容器
docker rm 容器id



#停止容器
docker stop 容器id/名字
#再次启动
docker start 容器id/名字
#修改配置开机自启
docker update 容器id/名字 --restart=always
```

## 2.3 修改容器内容

```bash
#进入容器 /bin/bash 命令行进入
[root@smc ~]# docker exec -it a9cb1b3e10ee /bin/bash
```

### 2.3.1 挂载数据到外部修改

```bash
#-v /data/html:/usr/share/nginx/html:ro
#ro表示只能外部写，rw表示内外部都可写
docker run --name testNginx --restart=always -d -p 88:80 -v /data/html:/usr/share/nginx/html:ro a759b5e83829
```



## 2.4提交改变

```bash
Usage:  docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]
Options:
  -a, --author string    Author (e.g., "John Hannibal Smith <hannibal@a-team.com>")
  -c, --change list      Apply Dockerfile instruction to the created image
  -m, --message string   Commit message
  -p, --pause            Pause container during commit (default true)


[root@smc ~]# docker commit -a "smchen" -m "提交测试" a9cb1b3e10ee smc_nginx:v1.0

```

### 2.4.1 保存镜像

```bash
[root@smc ~]# docker save --help

Usage:  docker save [OPTIONS] IMAGE [IMAGE...]

Save one or more images to a tar archive (streamed to STDOUT by default)

Options:
  -o, --output string   Write to a file, instead of STDOUT

[root@smc ~]# docker save -o abc.tar.gz smc_nginx:v1.0

  
[root@smc ~]# docker load --help

Usage:  docker load [OPTIONS]

Load an image from a tar archive or STDIN

Options:
  -i, --input string   Read from tar archive file, instead of STDIN
  -q, --quiet          Suppress the load output

[root@smc ~]# docker load -i abc.tar.gz 


```





## 2.5推送远程仓库

* 推送镜像到应用市场

```bash
#把旧镜像名字的名字，改成仓库要求的新镜像名字
docker tag local-image:tagname new-repo:tagname
docker push new-repo:tagname
```

```bash
docker tag smc_nginx:v1.0 smchen/my_images:smc_nginx1.0
docker push smchen/my_images:smc_nginx1.0

#首次推送需要登录
docker login/logout

```

## 2.6 常用命令

```bash
#查看docker运行日志
docker logs

#进入容器
docker exec -it 容器id /bin/bash

#文件复制，可双向
docker cp 容器id:/路径  本地路径
```

# 三、docker进阶

## 1、编写自己的应用,并打包

### 1.1 以前

* springboot打包成jar
* ` java -jar jar包`

### 1.2 现在

* 打包dockerfile

  ```dockerfile
  FROM openjdk:23-jdk-slim
  LABEL maintainer="smc"
  COPY ./docker-*.jar /app.jar
  
  ENTRYPOINT ["java", "-jar", "/app.jar"]
  ```

* 构建镜像

  ```bash
  [root@smc workplace]# docker build -t count_demo:v1.0 .
  ```

  

## 2、部署中间件redis

```bash
docker run --name myredis --restart=always -d -p 6379:6379 -v /data/conf:/usr/local/etc/redis redis redis-server /usr/local/etc/redis/redis.conf 镜像id
```

# 三、Kubernetes

## 1、基础概念

![image-20250216230951626](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/202502162348275.png)

> 大规模的容器编排系统

* Kubernetes 为你提供：

  - **服务发现和负载均衡**

    Kubernetes 可以使用 DNS 名称或自己的 IP 地址来暴露容器。 如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。

  - **存储编排**

    Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。

  - **自动部署和回滚**

    你可以使用 Kubernetes 描述已部署容器的所需状态， 它可以以受控的速率将实际状态更改为期望状态。 例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。

  - **自动完成装箱计算**

    你为 Kubernetes 提供许多节点组成的集群，在这个集群上运行容器化的任务。 你告诉 Kubernetes 每个容器需要多少 CPU 和内存 (RAM)。 Kubernetes 可以将这些容器按实际情况调度到你的节点上，以最佳方式利用你的资源。

  - **自我修复**

    Kubernetes 将重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器， 并且在准备好服务之前不将其通告给客户端。

  - **密钥与配置管理**

    Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。

  - **批处理执行** 除了服务外，Kubernetes 还可以管理你的批处理和 CI（持续集成）工作负载，如有需要，可以替换失败的容器。
  - **水平扩缩** 使用简单的命令、用户界面或根据 CPU 使用率自动对你的应用进行扩缩。
  - **IPv4/IPv6 双栈** 为 Pod（容器组）和 Service（服务）分配 IPv4 和 IPv6 地址。
  - **为可扩展性设计** 在不改变上游源代码的情况下为你的 Kubernetes 集群添加功能。

* Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移你的应用、提供部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary (金丝雀) 部署。

## 2、架构

### 2.1 工作方式

* Kubernetes Cluster=N Master Node + N Worker Node :N主节点+N工作节点；N>=1

### 2.2 组件架构

![image-20250217002743913](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250217082643775.png)

* 动画演示

![image-20250217233203690](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250217235025755.png)

## 3、kubeadm创建集群

![image-20250218000450565](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250218004224700.png)

### 3.1 先安装docker

### 3.2 安装kubelet、kubeadm、kubectl，版本1.28.2

* 一台兼容的Linux主机

* 每台机器2GB或更多的RAM（如果小于这个数字将会影响你应用的运行内存）

* 2CPU核或更多

* 集群中的所有机器的网络彼此聚能相互连接（公网和内网都可以）

  * 设置防火墙放行规则

  ```bash
  #关闭防火墙，关闭防火墙开机自启
  systemctl stop firewalld && systemctl disable firewalld
  ```

  

* 节点之中不可以用重复的主机名、MAC地址或product_uuid

  ```bash
  hostnamectl set-hostname 名称
  ```

* 开启机器上的某些端口
  * 内网互信
* 禁用交换分区。为了保证kubelet正常工作，你必须禁用交换分区
  * 永久关闭

```bash
#将SELinux设置为permissive模式（相当于将其禁用）
[root@k8s-node1 ~]# setenforce 0
[root@k8s-node1 ~]# sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# 关闭swap
[root@k8s-node1 ~]# swapoff -a
[root@k8s-node1 ~]# sed -ri 's/.*swap.*/#&/' /etc/fstab

#允许iptables 检查桥接流量
[root@k8s-node1 ~]# cat <<EOF | tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

[root@k8s-node1 ~]# cat << EOF | tee /etc/sysctl.d/k8s.conf 
net.bridge.bridge-nf-call-ip6tables=1
net.bridge.bridge-nf-call-iptables=1
net.ipv4.ip_forward = 1
EOF

[root@k8s-node1 ~]# sysctl --system

```

```bash
[root@k8s-node2 ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
> [kubernetes]
> name=Kubernetes
> baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
> enabled=1
> gpgcheck=1
> repo_gpgcheck=1
> gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
> EOF

# 由于kubernetes在1.23版本之后限制docker容器
[root@k8s-node2 ~]# yum install -y kubeadm-1.28.2 kubelet-1.28.2 kubectl-1.28.2 --disableexcludes=kubernetes

#设置kubelet开机自启
systemctl enble kubelet

```

* 修改hosts文件

  ```bash
  #在/etc/hosts中添加各节点映射
  192.168.17.143 smc
  192.168.17.142 k8s-node1
  192.168.17.144 k8s-node2
  ```

### 3.3 安装containerd

```bash
# 配置镜像源
yum install yum-utils device-mapper-persistent-data lvm2  -y
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 安装container
yum install  containerd.io -y

#修改container配置文件
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml
vim /etc/containerd/config.toml
#将SystemdCgroup = false修改为SystemdCgroup = true

#设置container开机自启
systemctl enable containerd  --now

#将 crictl 的配置文件写入 /etc/crictl.yaml，使其能够通过指定的 Unix 套接字与 containerd 进行通信，并设置了超时时间和调试模式
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
```

### 3.4 docker安装各个机器需要镜像

```bash
# kube-apiserver
docker pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.2
#kube-proxy
docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.28.2
#kube-controller-manager
docker pull registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.2
#kube-scheduler
docker pull registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.2
#coredns
docker pull registry.aliyuncs.com/google_containers/coredns:v1.10.1
#etcd
docker pull registry.aliyuncs.com/google_containers/etcd:3.5.9-0
#pause
docker pull registry.aliyuncs.com/google_containers/pause:3.9
#docker pull registry.aliyuncs.com/google_containers/pause:3.6

```

* 已打包好的镜像地址

  * [k8s-1.28.2.tar.gz](https://smcjava.oss-cn-hangzhou.aliyuncs.com/file/k8s%E9%95%9C%E5%83%8F%E5%8C%85/k8s-1-28.tar.gz)

  * [etcd.tar.gz](https://smcjava.oss-cn-hangzhou.aliyuncs.com/file/k8s%E9%95%9C%E5%83%8F%E5%8C%85/etcd.tar.gz)

  ```bash
  #将镜像包解压到k8s.io的namespace
  ctr -n=k8s.io images import k8s-1-28.tar.gz
  ctr -n=k8s.io images import etcd.tar.gz
  #查看置顶namespace的image
  ctr -n=k8s.io image list
  ```

### 3.5 初始化主节点

```bash
#将 crictl 的 runtime-endpoint 配置为 /run/containerd/containerd.sock，告诉 crictl 使用这个套接字与 containerd 进行通信
crictl config runtime-endpoint /run/containerd/containerd.sock

#查看kubernetes version
kubectl version

#主节点初始化,address配置为本机ip
kubeadm init --kubernetes-version=1.28.2  
--apiserver-advertise-address=192.168.17.143  
--image-repository registry.aliyuncs.com/google_containers  
--pod-network-cidr=10.244.0.0/16

#初始化若发生错误，查看日志
journalctl -xeu kubelet

#错误，failed to pull image registry.k8s.io/pause:3.6,部署worker的containerd时也要做相应修改
vim /etc/containerd/config.toml
#定位到 sandbox_image，将 仓库地址修改成 k8simage/pause:3.9
sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.9"
# 重启 containerd 服务
systemctl daemon-reload
systemctl restart containerd.service

#让当前用户能够使用kubectl与kubernetes集群交互
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#查看节点信息
kubectl get node

#根据配置文件，给集群创建资源
kubectl apply -f xxxx.yaml

# 查看集群部署了哪些应用。运行的应用在docker里面叫容器，在k8s里面叫pod
docker ps === kubectl get pods -A

# 查看pod日志
kubectl logs dashboard-metrics-scraper-84db5d9997-2nl6s -n kubernetes-dashboard

#查看pod启动描述
kubectl describe pod dashboard-metrics-scraper-84db5d9997-2nl6s -n kubernetes-dashboard

# 每个pod k8s都会分配一个ip
kubectl get pod -owide
#使用pod的ip+pod里面运行的容器的端口
```

```bash
Your Kubernetes control-plane has initialized successfully!
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

# 安装网络组件
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.17.143:6443 --token mbplyi.j4hd1c2mjixjk86g \
        --discovery-token-ca-cert-hash sha256:6ad603c977208458c7867f514d24de067a631dab4be9fb9dcdefb446ad84814b
```

### 3.6 安装网络组件

* [calico](https://www.tigera.io/project-calico/)
* [calico镜像](https://smcjava.oss-cn-hangzhou.aliyuncs.com/file/k8s%E9%95%9C%E5%83%8F%E5%8C%85/calico.tar.gz)

```bash
wget https://docs.projectcalico.org/manifests/calico.yaml

#去掉calico中的CALICO_IPV4POOL_CIDR的注释，并修改value为初始化时配置的值，统一网段
- name: CALICO_IPV4POOL_CIDR
	value: "10.244.0.0/16"

#手动推入镜像,推入到k8s.io的namespace
ctr -n=k8s.io images import calico.tar.gz
#安装
kubectl apply -f calico.yaml 

#查看所有pod
kubectl get pod -A
```

### 3.7加入worker节点

```bash
# 安装containerd,并进行配置
# containerd导入所有镜像,包括网络组件（如calico），etcd等
#将 crictl 的 runtime-endpoint 配置为 /run/containerd/containerd.sock，告诉 crictl 使用这个套接字与 containerd 进行通信
crictl config runtime-endpoint /run/containerd/containerd.sock

#在主节点生成token，将生成的命令在worker节点执行来加入集群
[root@smc data]# kubeadm token create --print-join-command
kubeadm join 192.168.17.143:6443 --token 3dzd5f.8jgzd6ass4jibxi5 --discovery-token-ca-cert-hash sha256:6ad603c977208458c7867f514d24de067a631dab4be9fb9dcdefb446ad84814b 

# 执行kubectl get node -A ，确保各个pod 都已ready
# watch -n 1 kubectl get pod -A  ,每秒监听一次命令
```

![image-20250225225417538](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250225225419562.png)

![image-20250225231002644](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250225231002830.png)

## 4、部署dashboard可视化界面

* [kubernetes提供的官方可是化界面](https://kubernetes.github.io/dashboard/)

### 4.1 离线版安装

* 安装heml
  * [heml下载地址](https://github.com/helm/helm/releases)
  * 解压：` tar -zxvf  helm-v3.7.1-linux-amd64.tar.gz`
  * 将helm移至bin目录：` mv  linux-amd64/helm  /usr/local/bin/helm`
  * 查看版本号：` helm version`

* [k8s 部署dashboard官网](https://kubernetes.io/zh-cn/docs/tasks/access-application-cluster/web-ui-dashboard/):：需要稳定连接外网拉取

* 手动拉取镜像后部署，**确保所有worker都导入了依赖的镜像，要不然是k8s指定的运行的worker没有image时会失败**,` kubectl apply -f kubernetes-dashboard.yaml`

  ```yaml
  # Copyright 2017 The Kubernetes Authors.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  
  apiVersion: v1
  kind: Namespace
  metadata:
    name: kubernetes-dashboard
  
  ---
  
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard
  
  ---
  
  kind: Service
  apiVersion: v1
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard
  spec:
    ports:
      - port: 443
        targetPort: 8443
    selector:
      k8s-app: kubernetes-dashboard
  
  ---
  
  apiVersion: v1
  kind: Secret
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard-certs
    namespace: kubernetes-dashboard
  type: Opaque
  
  ---
  
  apiVersion: v1
  kind: Secret
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard-csrf
    namespace: kubernetes-dashboard
  type: Opaque
  data:
    csrf: ""
  
  ---
  
  apiVersion: v1
  kind: Secret
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard-key-holder
    namespace: kubernetes-dashboard
  type: Opaque
  
  ---
  
  kind: ConfigMap
  apiVersion: v1
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard-settings
    namespace: kubernetes-dashboard
  
  ---
  
  kind: Role
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard
  rules:
    # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
    - apiGroups: [""]
      resources: ["secrets"]
      resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]
      verbs: ["get", "update", "delete"]
      # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
    - apiGroups: [""]
      resources: ["configmaps"]
      resourceNames: ["kubernetes-dashboard-settings"]
      verbs: ["get", "update"]
      # Allow Dashboard to get metrics.
    - apiGroups: [""]
      resources: ["services"]
      resourceNames: ["heapster", "dashboard-metrics-scraper"]
      verbs: ["proxy"]
    - apiGroups: [""]
      resources: ["services/proxy"]
      resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]
      verbs: ["get"]
  
  ---
  
  kind: ClusterRole
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
  rules:
    # Allow Metrics Scraper to get metrics from the Metrics server
    - apiGroups: ["metrics.k8s.io"]
      resources: ["pods", "nodes"]
      verbs: ["get", "list", "watch"]
  
  ---
  
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: kubernetes-dashboard
  subjects:
    - kind: ServiceAccount
      name: kubernetes-dashboard
      namespace: kubernetes-dashboard
  
  ---
  
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: kubernetes-dashboard
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: kubernetes-dashboard
  subjects:
    - kind: ServiceAccount
      name: kubernetes-dashboard
      namespace: kubernetes-dashboard
  
  ---
  
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard
  spec:
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kubernetes-dashboard
    template:
      metadata:
        labels:
          k8s-app: kubernetes-dashboard
      spec:
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        containers:
          - name: kubernetes-dashboard
            image: registry.cn-hangzhou.aliyuncs.com/google_containers/dashboard:v2.7.0
            imagePullPolicy: Never
            ports:
              - containerPort: 8443
                protocol: TCP
            args:
              - --auto-generate-certificates
              - --namespace=kubernetes-dashboard
              # Uncomment the following line to manually specify Kubernetes API server Host
              # If not specified, Dashboard will attempt to auto discover the API server and connect
              # to it. Uncomment only if the default does not work.
              # - --apiserver-host=http://my-address:port
            volumeMounts:
              - name: kubernetes-dashboard-certs
                mountPath: /certs
                # Create on-disk volume to store exec logs
              - mountPath: /tmp
                name: tmp-volume
            livenessProbe:
              httpGet:
                scheme: HTTPS
                path: /
                port: 8443
              initialDelaySeconds: 30
              timeoutSeconds: 30
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              runAsUser: 1001
              runAsGroup: 2001
        volumes:
          - name: kubernetes-dashboard-certs
            secret:
              secretName: kubernetes-dashboard-certs
          - name: tmp-volume
            emptyDir: {}
        serviceAccountName: kubernetes-dashboard
        nodeSelector:
          "kubernetes.io/os": linux
        # Comment the following tolerations if Dashboard must not be deployed on master
        tolerations:
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
  
  ---
  
  kind: Service
  apiVersion: v1
  metadata:
    labels:
      k8s-app: dashboard-metrics-scraper
    name: dashboard-metrics-scraper
    namespace: kubernetes-dashboard
  spec:
    ports:
      - port: 8000
        targetPort: 8000
    selector:
      k8s-app: dashboard-metrics-scraper
  
  ---
  
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    labels:
      k8s-app: dashboard-metrics-scraper
    name: dashboard-metrics-scraper
    namespace: kubernetes-dashboard
  spec:
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: dashboard-metrics-scraper
    template:
      metadata:
        labels:
          k8s-app: dashboard-metrics-scraper
      spec:
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        containers:
          - name: dashboard-metrics-scraper
            image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-scraper:v1.0.8
            imagePullPolicy: Never
            ports:
              - containerPort: 8000
                protocol: TCP
            livenessProbe:
              httpGet:
                scheme: HTTP
                path: /
                port: 8000
              initialDelaySeconds: 30
              timeoutSeconds: 30
            volumeMounts:
            - mountPath: /tmp
              name: tmp-volume
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              runAsUser: 1001
              runAsGroup: 2001
        serviceAccountName: kubernetes-dashboard
        nodeSelector:
          "kubernetes.io/os": linux
        # Comment the following tolerations if Dashboard must not be deployed on master
        tolerations:
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
        volumes:
          - name: tmp-volume
            emptyDir: {}
  ```

![image-20250227095140081](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250227095140495.png)

### 4.2 设置访问端口

```bash
kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
```

> type: ClusterIP 改为 type: NodePort

```bash
kubect1 get svc -A | grep kubernetes-dashboard
#确保显示的端口在防火墙放行
```

![image-20250227095527683](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250227095527851.png)

* 访问：https://集群任意ip:端口 https://192.168.17.144:32636

### 4.3 创建访问账号

* [帮助文档](https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md)

```yaml
# dash-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: admin-user
    namespace: kubernetes-dashboard
---
apiVersion: v1
kind: Secret
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/service-account.name: "admin-user"   
type: kubernetes.io/service-account-token  

```

```bash
kubectl apply -f dash-admin.yaml
```

### 4.4 获取访问令牌

```bash
#获取临时token
kubectl -n kubernetes-dashboard create token admin-user

#获取长期token
kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath="{.data.token}" | base64 -d

#清理管理员ServoceAccount和ClusterRoleBinding
kubectl -n kubernetes-dashboard delete serviceaccount admin-user
kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user
```

```bash
eyJhbGciOiJSUzI1NiIsImtpZCI6IlphZWxCWllSM3JBZHRScFRuc3FjWjhRemRBTDd3elFybXBKSERWZ3R2VkkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI4ZjExOGI5Zi1jZGNiLTQxMDMtYTUyYi02Zjc4Njk5ZDcyNTIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.w6fWZdI2PufmedaZ5AY_SzzmC_mE-9_MiU2YASLzifwL91dcrj6ojd6z7nzgNmAddmIfss6507dlshDy_OSIdWR82RsxddrzqvYu3AH_CNQvlKWIwE_8OfoX9zLVE-LszQ00ZGneDAeQ1HIJckbjdujsyWIizIk6Z6p2Kqdt2e5lhgxzTPE7b9rUYwTWaalooQtbVL7knIasLlYCp8xiyqQH--iNxA8eqBNk2YEl7K40vEXw72zQQP2UEX6jS2giuLZT6Ryu3GX_voINUDHQ-Gje5rdWe5AAePa2LMHM80viT6doFuft55rafqtVGgOBW9dVgaM9ZLVZVJ4oe4L-4Q
```

* 设置过期时间

  ```bash
  #修改yaml文件
  kubectl -n kubernetes-dashboard edit deployment kubernetes-dashboard
  #在arg下配置过期时间，12小时
  - --token-ttl=43200
  #重启pod
  kubectl -n kubernetes-dashboard rollout restart deployment kubernetes-dashboard
  ```

  ![image-20250302130519463](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250302130519574.png)

# 5、kubernetes核心实战

### 5.1 资源创建方式

* 命令行
* YAML

### 5.2 NameSpace

* 用来隔离资源，默认只隔离资源，不隔离网络

```bash
#创建名称空间
kubectl create ns hello
#删除名称空间
kubectl delete ns hello
```

* 通过yaml配置文件来创建名称空间

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: <insert-namespace-name-here>
```

```bash
#根据配置文件新增资源
kubectl apply -f ns.yaml
#根据配置文件，将之前创建的资源删除
kubectl delete -f ns.yaml
```

### 5.3 Pod

* 运行中的一组容器，Pod是kubernetes中应用的最小单位。
* 一个Pod可以有多个容器

![image-20250227201255370](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250227201255646.png)

* 命令行创建Pod

```bash
#使用镜像nginx创建pod mynginx，默认都是拉取
kubectl run mynginx --image=nginx
```

* 配置文件生成Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: mynginx
  name: mynginx
spec:
  containers:
  - name: mynginx-container
    image: docker.dark-nt.us.kg/library/nginx:latest
    imagePullPolicy: IfNotPresent
```

* 可视化创建Pod，当为选择namespace时，需要在yaml文件中指定namespace

![image-20250301014216850](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250301014217052.png)

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: mynginx
  name: mynginx
  namespace: default #指定名称空间
spec:
  containers:
  - name: mynginx-container
    image: docker.dark-nt.us.kg/library/nginx:latest
    imagePullPolicy: IfNotPresent
```

* 一个pod里面有多个容器

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    labels:
      run: myapp
    name: myapp
    namespace: default #指定名称空间
  spec:
    containers:
    - name: myapp-nginx
      image: docker.dark-nt.us.kg/library/nginx:latest
      imagePullPolicy: IfNotPresent
    - name: myapp-tomcat
      image: docker.dark-nt.us.kg/library/tomcat:8.5.68
      imagePullPolicy: IfNotPresent
  ```

  * 在同一个pod里面的容器可以使用127.0.0.1或者服务名进行相互访问
  * ![image-20250301114318728](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250301114319007.png)
  * 在一个pod里面端口不能共用

### 5.4 Deployment

* 控制Pod、使Pod拥有多副本，自愈、扩缩容等能力

  * ```bash
    #可直接删除，而不会回复
    kubectl run mynginx --image=docker.dark-nt.us.kg/library/nginx:latest
    kubectl delete pod mynginx
    
    #使用deployment创建的Pod，删除后k8s会帮助回复，也就是自愈性
    kubectl create deploy mytomcat --image=docker.dark-nt.us.kg/library/tomcat:8.5.68
    #使用kubectl delete pod podname删除时，k8s会帮助回复，也就是自愈性
    #需要删除部署才能真正删除部署
    kubectl get deploy
    kubectl delete deploy mytomcat
    ```

* 多副本部署

  * ```bash
    #命令行部署
    kubectl create deploy mynginx --image=docker.dark-nt.us.kg/library/nginx:latest --replicas=3
    ```

  * 可视化表单部署

    ![image-20250302120240486](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250302120240749.png)

  * 配置文件部署

    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: my-dep
      name: my-dep
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: my-dep
      template:
        metadata:
          labels:
            app: my-dep
        spec:
          containers:
          - name: mynginx-container
            image: docker.dark-nt.us.kg/library/nginx:latest
            imagePullPolicy: IfNotPresent
    ```

* 扩缩容

  * 命令行扩缩容

    ```bash
    kubectl scale deploy/my-dep --replicas=4
    ```

  * 配置文件扩缩容

    ```bash
    kubectl edit my-dep
    ```

  * 可视化配置界面

    ![image-20250302150839166](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250302150839587.png)

* 自愈&故障转移

  * 当服务器或者pod故障时，k8s总会启动新的pod，保证depoy的设置
  * 可设置故障鉴定判定时间

* 滚动更新，不停机更新

  ```bash
  #获取容器名称
  kubectl get deploy my-dep -oyaml
  #获取获取容器名称进行替换，并且使用--record记录更新历史
  kubectl set image deploy/my-dep mynginx-container=docker.dark-nt.us.kg/library/nginx:1.16.1 --record
  ```

* 版本回退

  ```bash
  #获取更新记录
  [root@smc ~]# kubectl rollout history deploy/my-dep
  deployment.apps/my-dep 
  REVISION  CHANGE-CAUSE
  1         kubectl set image deploy/my-dep mynginx-container:latest=docker.dark-nt.us.kg/library/nginx:1.16.1 --record=true
  2         kubectl set image deploy/my-dep mynginx-container=docker.dark-nt.us.kg/library/nginx:1.16.1 --record=true
  
  [root@smc ~]# kubectl rollout undo deploy/my-dep --to-revision=1
  ```

* 更多

  > 除了Deployment，k8s还有StatefulSet、DaemonSet、Job等类型资源。我们都称为工作负载。
  >
  > 有状态应用使用StatefulSet部署，无状态应用使用Deployment部署

  ![image-20250302161329576](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250302161330050.png)

  

### 5.5 Service

* Pod 的服务发现与负载均衡

  ![image-20250303010118617](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250303010118806.png)

  * 命令行创建

  ```bash
  #创建service
  kubectl expose deploy my-dep --port=8001 --target-port=80
  #查看service服务列表
  [root@smc ~]# kubectl get service
  NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
  kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP    2d11h
  my-dep       ClusterIP   10.98.200.118   <none>        8001/TCP   22s
  
  #删除service
  kubectl delete svc my-dep
  
  
  
  ```

  * 配置表创建

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: my-dep
    name: my-dep
  spec:
    selector:
      matchLabels:
        app: my-dep
    ports:
    -	port: 8001
    	protocol: TCP
    	targetPort: 80
  ```

  * 公网访问

  ```bash
  #只能内网访问
  kubectl expose deploy my-dep --port=8001 --target-port=80 --type=ClusterIP
  #公网访问
  kubectl expose deploy my-dep --port=8001 --target-port=80 --type=NodePort
  #Node范围在30000~32767之间,各节点ip均可负载均衡访问
  [root@smc ~]# kubectl get svc
  NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
  kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP          2d13h
  my-dep       NodePort    10.108.68.195   <none>        8001:31666/TCP   6s
  
  ```

### 5.6 Ingress

* Service的统一网关入口

![image-20250303204238336](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250303204238597.png)

* 官网地址:[https://kubernetes.github.io/ingress-nginx/deploy/](https://kubernetes.github.io/ingress-nginx/deploy/)

```bash
#将配置文件下载下来，调整yaml文件
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.0/deploy/static/provider/cloud/deploy.yaml

#如果apply无法正常拉取镜像，可手动拉取

```

* admission-create和admission-patch只在刚开始运行来负责创建文件，后面都是complete状态

![image-20250304005452347](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250304005542397.png)![image-20250304005648920](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250304005649031.png)

>http://节点ip:31450
>
>https://节点ip:31128

* 搭建service测试环境

  * 启动两个测试deployment

    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: nginx-demo
      name: nginx-demo
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx-demo
      template:
        metadata:
          labels:
            app: nginx-demo
        spec:
          containers:
          - name: mynginx-container
            image: docker.dark-nt.us.kg/library/nginx:latest
            imagePullPolicy: IfNotPresent
    ---
    
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: tomcat-demo
      name: tomcat-demo
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: tomcat-demo
      template:
        metadata:
          labels:
            app: tomcat-demo
        spec:
          containers:
          - name: mytomcat-container
            image: docker.dark-nt.us.kg/library/tomcat:8.5.68
            imagePullPolicy: IfNotPresent
    ```

  * 新建service

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: nginx-demo
      name: nginx-demo
    spec:
      selector:
        app: nginx-demo
      ports:
      - port: 8000
        protocol: TCP
        targetPort: 80
    
    ---
    
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: tomcat-demo
      name: tomcat-demo
    spec:
      selector:
        app: tomcat-demo
      ports:
      - port: 8000
        protocol: TCP
        targetPort: 8080
    ```

* 域名访问

  ```yaml
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: my-test-ingress
    namespace: default
  spec:
    ingressClassName: nginx  # 指定 Ingress 控制器
    rules:
    - host: nginx.demo.com  # 绑定的域名
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: nginx-demo  # 关联的服务名称
              port:
                number: 8000  # 服务端口
  	- host: tomcat.demo.com  # 绑定的域名
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: tomcat-demo  # 关联的服务名称
              port:
                number: 8000  # 服务端口
  ```

* 路径重写

  ```yaml
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    annotations:
      nginx.ingress.kubernetes.io/use-regex: "true"
      nginx.ingress.kubernetes.io/rewrite-target: /$2
    name: my-test-ingress
    namespace: default
  spec:
    ingressClassName: nginx  # 指定 Ingress 控制器
    rules:
    - host: nginx.demo.com  # 绑定的域名
      http:
        paths:
        - path: /nginx(/|$)(.*)
          pathType: ImplementationSpecific
          backend:
            service:
              name: nginx-demo  # 关联的服务名称
              port:
                number: 8000  # 服务端口
    - host: tomcat.demo.com  # 绑定的域名
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: tomcat-demo  # 关联的服务名称
              port:
                number: 8000  # 服务端口
  ```

  > `nginx.demo.com/nginx` rewrites to `nginx.demo.com/nginx/`
  >
  > `nginx.demo.com/nginx/` rewrites to `nginx.demo.com/nginx/`
  >
  > `nginx.demo.com/nginx/new` rewrites to `nginx.demo.com/nginx/new`

* 流量限制

  ```yaml
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    annotations:
      nginx.ingress.kubernetes.io/limit-rps: "1" #流量限制为1
    name: rete-limit-ingress
    namespace: default
  spec:
    ingressClassName: nginx  # 指定 Ingress 控制器
    rules:
    - host: limit-nginx.demo.com  # 绑定的域名
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: nginx-demo  # 关联的服务名称
              port:
                number: 8000  # 服务端口
  ```

  ![image-20250305235320320](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250305235321313.png)

* 网络模型

![image-20250305235606507](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250305235606707.png)

### 5.7 存储抽象

* 有很多种网络存储系统，如Glusterfs，NFS，CephFS等，现在以NFS为例

![image-20250306000010232](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250306000010875.png)

#### 5.7.1 NFS 网络文件系统

* 环境准备

  ```yaml
  #所有机器安装nfs工具
  yum install -y nfs-utils
  ```

* 主节点

  ```bash
  echo "/nfs/data/ *(insecure,rw,sync,no_root_squash)" > /etc/exports
  
  mkdir -p /nfs/data
  systemctl enable rpcbind --now
  systemctl enable nfs-server --now
  #配置生效
  exportfs -r
  [root@smc data]# exportfs
  /nfs/data       <world>
  
  ```

* 从节点

  ```bash
  #根据ip查看挂载目录
  [root@k8s-node1 ~]# showmount -e 192.168.17.143
  Export list for 192.168.17.143:
  /nfs/data *
  
  #执行命令挂载nfs服务器上的共享目录到本机路径 /nfs/data
  [root@k8s-node1 ~]# mkdir -p /nfs/data
  [root@k8s-node1 ~]# mount -t nfs 192.168.17.143:/nfs/data /nfs/data
  #写入测试文件
  [root@smc ~]# echo "hello nfs server" >/nfs/data/test.txt
  ```

#### 5.7.2 原生方式数据挂载

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-pv
  name: nginx-pv
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-pv
  template:
    metadata:
      labels:
        app: nginx-pv
    spec:
      containers:
      - name: mynginx-container
        image: docker.dark-nt.us.kg/library/nginx:latest
        imagePullPolicy: IfNotPresent
        volumeMounts:
        -	name: html
        	mountPath: /usr/share/nginx/html #容器内路径
      volumes:
      	-	name: html
      		nfs:
      			server: 192.168.17.143
      			path: /nfs/data/nginx-pv
        	
```

#### 5.7.3 PV&PVC

* PV:持久卷（Persistent Volume）：将应用需要持久化的数据保存到指定为止
* PV:持久卷生命（Persistent Volume Claim）,申明需要使用的持久卷规格

![image-20250307002703575](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250307002704485.png)

* 创建PV池

  * 静态供应

    >mkdri -p /nfs/data/01 /nfs/data/02

  * 创建PV

    ```yaml
    metadata:
      name: pv01-10m
    spec:
      capacity:
        storage: 10M
      accessModes:
        - ReadWriteMany
      storageClassName: nfs
      nfs:
        path: /nfs/data/01
        server: 192.168.17.143
    ---
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: pv02-1g
    spec:
      capacity:
        storage: 1G
      accessModes:
        - ReadWriteMany
      storageClassName: nfs
      nfs:
        path: /nfs/data/02
        server: 192.168.17.143
    ---
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: pv03-3g
    spec:
      capacity:
        storage: 3G
      accessModes:
        - ReadWriteMany
      storageClassName: nfs
      nfs:
        path: /nfs/data/03
        server: 192.168.17.143
    ```

    > kubectl get pv

  * PVC的声明与绑定

    ```yaml
    ---
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: nginx-pvc
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 200M
      storageClassName: nfs
    ```

    ```bash
    [root@smc data]# kubectl get pvc
    NAME        STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
    nginx-pvc   Bound    pv02-1g   1G         RWX            nfs            17s
    [root@smc data]# kubectl get pv
    NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM               STORAGECLASS   REASON   AGE
    pv01-10m   10M        RWX            Retain           Available                       nfs                     34m
    pv02-1g    1G         RWX            Retain           Bound       default/nginx-pvc   nfs                     9m45s
    pv03-3g    3G         RWX            Retain           Available                       nfs                     9m45s
    ```

  * 创建Pod绑定PVC

    ```yaml
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: nginx-deploy-pvc
      name: nginx-deploy-pvc
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx-deploy-pvc
      template:
        metadata:
          labels:
            app: nginx-deploy-pvc
        spec:
          containers:
          - name: mynginx-container
            image: docker.dark-nt.us.kg/library/nginx:latest
            imagePullPolicy: IfNotPresent
            volumeMounts:
            - name: html
              mountPath: /usr/share/nginx/html      # 容器内路径
          volumes:
            - name: html
              persistentVolumeClaim:
                claimName: nginx-pvc
    
    ```

#### 5.7.4 ConfigMap

* 抽取应用配置，并且自动更新

* redis示例

  * 把之前的配置文件创建为配置集

    ```bash
    #创建配置，redis保存到k8s的etcd
    [root@smc data]# kubectl create cm redis-conf --from-file=redis.conf 
    configmap/redis-conf created
    [root@smc data]# kubectl get cm
    NAME               DATA   AGE
    kube-root-ca.crt   1      10d
    redis-conf         1      7s
    [root@smc data]# kubectl get cm redis-conf -oyaml
    apiVersion: v1
    data: 		#data是配置文件的真正内容，key是文件名，value是文件内容
      redis.conf: |
        appendonly yes
    kind: ConfigMap
    metadata:
      creationTimestamp: "2025-03-06T20:56:38Z"
      name: redis-conf
      namespace: default
      resourceVersion: "457062"
      uid: 06306887-c510-4b31-b4c6-d7fc2f9a1cfc
    
    ```

  * 创建pod

    ```yaml
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: redis-config
    spec:
      containers:
        - name: redis
          image: docker.dark-nt.us.kg/library/redis:latest
          imagePullPolicy: IfNotPresent
          command:
            - redis-server
            - "/redis-master/redis.conf"  # 指定是redis容器内部存放配置文件的位置
          ports:
            - containerPort: 6379
          volumeMounts:
            - name: data
              mountPath: /data  # 容器内路径
            - name: config
              mountPath: /redis-master
      volumes:  # 外面挂载位置
        - name: data
          emptyDir: {}
        - name: config
          configMap:
            name: redis-conf
            items:
              - key: redis.conf
                path: redis.conf
    ```

  ```bash
  #修改configmap之后，pod里面的配置文件也会同步获取
  [root@smc data]# kubectl edit cm redis-conf
  
  #但是pod的配置功能并未生效，pod并没有热更新的功能，需要重启pod生效
  ```

#### 5.7.5 Secret

* Secret对象类型用来保存敏感信息，例如密码、OAuth令牌和SSH密钥。将这些信息放在secret中比放在Pod的定义或者容器镜像中来说更加安全和灵活

  ```bash
  kubectl create secret docker-registry my-docker-secret \
    --docker-server=https://index.docker.io/v1/ \
    --docker-username=myuser \
    --docker-password=mypassword \
    --docker-email=myemail@example.com
    
  kubectl create secret docker-registry smchen-docker-secret \
    --docker-username=smchen \
    --docker-password=smc06979329 \
    --docker-email=smc950910@163.com
  ```

  ```yaml
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: redis-secret
  spec:
    containers:
      - name: redis
        image: smchen/my_images:redis1.0
        imagePullPolicy: IfNotPresent
    imagePullSecrets:
      - name: smchen-docker-secret
  
  ```

  

### QA

* 当linux服务器设置了代理时，可能导致服务器内根据自己ip访问pod失败，会被代理强行访问

  * ![image-20250301122219102](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250301122219524.png)

  ```bash
  # 解决方法
  # 1、可在~/.bashrc中设置no_proxy，保证访问ip不被代理
  export no_proxy="127.0.0.1,localhost,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,svc,.cluster.local"
  
  # 2、在访问命令加入不被代理参数 --noproxy '*'
  curl -v --noproxy '*' http://10.244.36.83:80
  
  
  #当无法正确拉取镜像时，查看是否DNS存在异常
  nslookup docker.dark-nt.us.kg
  dig docker.dark-nt.us.kg
  #如果返回 connection timed out，表示 DNS 解析失败，可能是 /etc/resolv.conf 里 DNS 服务器配置错误。
  #如果 SERVFAIL 或 NXDOMAIN，表示 docker.dark-nt.us.kg 可能在 DNS 服务器上没有解析记录。
  #尝试手动指定DNS
  nslookup docker.dark-nt.us.kg 8.8.8.8
  #如果 8.8.8.8 能解析，但默认 DNS 不能，那可能是 192.168.17.2（集群内 DNS 服务器）有问题。
  #临时修复
  echo "nameserver 8.8.8.8" > /etc/resolv.conf
  
  ```

# 四、Kubesphere

![image-20250307171120900](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250307171121763.png)

* [官网地址](https://www.kubesphere.io/zh/)

## 1、在Kubernetes上安装

### 1.1 部署k8s

### 1.2 部署nfs动态存储空间

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "true"
---
apiVersion: v1
kind: Namespace
metadata:
  name: nfs-provisioner
---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  namespace: nfs-provisioner
  labels:
    app: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          imagePullPolicy: IfNotPresent
          #resources:
          #  limits:
          #    cpu: "100m"
          #  requests:
          #    cpu: "50m"
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: "192.168.17.143"  # 修改为你的 NFS 服务器地址
            - name: NFS_PATH
              value: "/nfs/data"  # 修改为你的 NFS 共享路径
      volumes:
        - name: nfs-client-root
          nfs:
            server: "192.168.17.143"
            path: "/nfs/data"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  namespace: nfs-provisioner

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch", "create", "delete", "patch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["watch", "create", "update", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: nfs-provisioner
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: nfs-provisioner
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: nfs-provisioner
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io

```

* 配置pvc绑定

  ```yaml
  ---
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: external-provisioner-pvc
  spec:
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: 200M
  ```

  ```bash
  [root@smc data]# kubectl get pvc
  NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
  external-provisioner-pvc   Bound    pvc-6aaf2bda-0887-4958-b909-1217377f547a   200M       RWX            nfs-storage    5s
  #动态给予了pv空间
  [root@smc data]# kubectl get pv
  NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS   REASON   AGE
  pvc-6aaf2bda-0887-4958-b909-1217377f547a   200M       RWX            Delete           Bound    default/external-provisioner-pvc   nfs-storage             10s
  [root@smc data]# kubectl get pv
  NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS   REASON   AGE
  pvc-6aaf2bda-0887-4958-b909-1217377f547a   200M       RWX            Delete           Bound    default/external-provisioner-pvc   nfs-storage             18s
  
  ```

### 1.3 安装metrics-server

* 集群指标监控组件

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: system:aggregated-metrics-reader
rules:
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
rules:
- apiGroups:
  - ""
  resources:
  - nodes/metrics
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    k8s-app: metrics-server
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  strategy:
    rollingUpdate:
      maxUnavailable: 0
  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=10250
        - --kubelet-insecure-tls
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.7.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /livez
            port: https
            scheme: HTTPS
          periodSeconds: 10
        name: metrics-server
        ports:
        - containerPort: 10250
          name: https
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /readyz
            port: https
            scheme: HTTPS
          initialDelaySeconds: 20
          periodSeconds: 10
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /tmp
          name: tmp-dir
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      serviceAccountName: metrics-server
      volumes:
      - emptyDir: {}
        name: tmp-dir
---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  labels:
    k8s-app: metrics-server
  name: v1beta1.metrics.k8s.io
spec:
  group: metrics.k8s.io
  groupPriorityMinimum: 100
  insecureSkipTLSVerify: true
  service:
    name: metrics-server
    namespace: kube-system
  version: v1beta1
  versionPriority: 100

```

```bash
#查看占用cpu和内存占用
[root@smc data]# kubectl top nodes
NAME        CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-node1   406m         40%    1216Mi          54%       
k8s-node2   378m         37%    1181Mi          53%       
smc         1002m        50%    2131Mi          58% 
```

### 1.4 安装Kubesphere

* [通过Helm安装](https://www.kubesphere.io/zh/docs/v4.1/02-quickstart/01-install-kubesphere/)

  ```bash
  helm upgrade --install -n kubesphere-system --create-namespace ks-core https://charts.kubesphere.io/main/ks-core-1.1.3.tgz --debug --wait --set global.imageRegistry=swr.cn-southwest-2.myhuaweicloud.com/ks --set extension.imageRegistry=swr.cn-southwest-2.myhuaweicloud.com/ks
  
  #卸载
  [root@smc data]# helm -n kubesphere-system uninstall ks-core
  ```

* 安装成功信息

  ```bash
  NOTES:
  Thank you for choosing KubeSphere Helm Chart.
  
  Please be patient and wait for several seconds for the KubeSphere deployment to complete.
  
  1. Wait for Deployment Completion
  
      Confirm that all KubeSphere components are running by executing the following command:
  
      kubectl get pods -n kubesphere-system
  2. Access the KubeSphere Console
  
      Once the deployment is complete, you can access the KubeSphere console using the following URL:  
  
      http://192.168.17.143:30880
  
  3. Login to KubeSphere Console
  
      Use the following credentials to log in:
  
      Account: admin
      Password: P@88w0rd
  
  NOTE: It is highly recommended to change the default password immediately after the first login.
  
  ```

# 五、DevOps

## 1、DevOps基础

* DevOps是一系列做法和工具，可以使用IT和软件开发团队之间的流程实现自动糊啊，随着敏捷软件开发日趋流行，持续集成（CI）和持续交付（CD）已经成为该领域一个理想的解决方法。在CI/CD工作流中，每次集成都通过自动化构建来验证，包括编码、发布和测试，从而帮助开发者提前发现集成错误，团队也可以快速、安全、可靠地内部软件交付到生产环境
* [https://www.kubesphere.io/zh/docs/v4.1/11-use-extensions/00-overview/](https://www.kubesphere.io/zh/docs/v4.1/11-use-extensions/00-overview/)

![image-20250309151549430](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250309151550726.png)

## 2、尚医通项目云部署

![image-20250309152556807](https://smcjava.oss-cn-hangzhou.aliyuncs.com/java/20250309152557167.png)

### 2.1 项目地址

* [https://gitee.com/sumonton/yygh-parent.git](https://gitee.com/sumonton/yygh-parent.git)
* [https://gitee.com/sumonton/yygh-admin.git](https://gitee.com/sumonton/yygh-admin.git)

* [https://gitee.com/sumonton/yygh-site.git](https://gitee.com/sumonton/yygh-site.git)

### 2.2 中间件

| 中间件        | 集群内地址               | 外部访问地址                 |
| ------------- | ------------------------ | ---------------------------- |
| Nacos         |                          |                              |
| MySQL         |                          |                              |
| Redis         |                          |                              |
| Sentinel      | http://sentinel.his:8080 | http://192.168.17.143:30974/ |
| MongoDB       |                          |                              |
| RabbitMQ      |                          |                              |
| ElasticSearch |                          |                              |
|               |                          |                              |





